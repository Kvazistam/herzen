{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лабораторная работа: Реализация алгоритма обратного распространения ошибки (Шаблон)\n",
    "\n",
    "**Цель:** Реализовать с нуля полносвязную нейронную сеть на Python с использованием объектно-ориентированного подхода и обучить её с помощью алгоритма обратного распространения ошибки на мини-батчах.\n",
    "\n",
    "**Задачи:**\n",
    "1. Реализовать класс `NeuralNetwork`.\n",
    "2. Реализовать функции активации (Sigmoid, ReLU, Tanh) и их производные.\n",
    "3. **ЗАПОЛНИТЬ:** Реализовать методы прямого распространения (`_forward`).\n",
    "4. **ЗАПОЛНИТЬ:** Реализовать метод вычисления функции потерь (`_compute_cost`).\n",
    "5. **ЗАПОЛНИТЬ:** Реализовать метод обратного распространения ошибки (`_backward`).\n",
    "6. **ЗАПОЛНИТЬ:** Реализовать метод обновления параметров (`_update_parameters`).\n",
    "7. **ЗАПОЛНИТЬ:** Реализовать метод обучения (`fit`) с использованием мини-батчей.\n",
    "8. **ЗАПОЛНИТЬ:** Реализовать метод предсказания (`predict`).\n",
    "9. Протестировать реализацию на простом наборе данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Импорт библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons, make_circles, make_blobs\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Реализация вспомогательных функций и класса нейронной сети\n",
    "\n",
    "Ниже приведены вспомогательные функции активации и структура класса `NeuralNetwork`. Вам необходимо заполнить пропущенные части в методах класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================\n",
    "# Вспомогательные функции активации\n",
    "# ====================================\n",
    "\n",
    "def sigmoid(z):\n",
    "    \"\"\"Сигмоидная функция активации.\"\"\"\n",
    "    # Добавляем защиту от переполнения\n",
    "    z = np.clip(z, -500, 500)\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Производная сигмоидной функции.\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def relu(z):\n",
    "    \"\"\"ReLU функция активации.\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Производная ReLU функции.\"\"\"\n",
    "    return np.where(z > 0, 1.0, 0.0) # Используем float\n",
    "\n",
    "def tanh(z):\n",
    "    \"\"\"Гиперболический тангенс функция активации.\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"Производная гиперболического тангенса.\"\"\"\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "# Словарь для доступа к функциям активации и их производным по имени\n",
    "ACTIVATIONS = {\n",
    "    'sigmoid': (sigmoid, sigmoid_derivative),\n",
    "    'relu': (relu, relu_derivative),\n",
    "    'tanh': (tanh, tanh_derivative)\n",
    "}\n",
    "\n",
    "# ====================================\n",
    "# Класс нейронной сети\n",
    "# ====================================\n",
    "\n",
    "class NeuralNetwork:\n",
    "    \"\"\"Класс полносвязной нейронной сети с обучением по mini-batches.\"\"\"\n",
    "    \n",
    "    def __init__(self, layer_dims, activation_funcs, learning_rate=0.01):\n",
    "        \"\"\"Инициализация нейронной сети.\n",
    "\n",
    "        Args:\n",
    "            layer_dims (list): Список с количеством нейронов в каждом слое \n",
    "                               (включая входной и выходной).\n",
    "                               Пример: [input_size, hidden1_size, hidden2_size, output_size]\n",
    "            activation_funcs (list): Список с названиями функций активации для каждого \n",
    "                                     скрытого и выходного слоя (длина = len(layer_dims) - 1).\n",
    "                                     Пример: ['relu', 'relu', 'sigmoid']\n",
    "            learning_rate (float): Скорость обучения.\n",
    "        \"\"\"\n",
    "        if len(layer_dims) < 2:\n",
    "            raise ValueError(\"Сеть должна иметь как минимум входной и выходной слои.\")\n",
    "        if len(activation_funcs) != len(layer_dims) - 1:\n",
    "            raise ValueError(\"Количество функций активации должно соответствовать количеству слоев (исключая входной).\")\n",
    "            \n",
    "        self.num_layers = len(layer_dims)\n",
    "        self.layer_dims = layer_dims\n",
    "        self.learning_rate = learning_rate\n",
    "        self.parameters = {} # Словарь для хранения весов (W) и смещений (b)\n",
    "        self.activation_funcs = {} # Словарь для хранения функций активации и их производных\n",
    "\n",
    "        # Инициализация весов и смещений\n",
    "        np.random.seed(42) # Для воспроизводимости\n",
    "        for l in range(1, self.num_layers):\n",
    "            # Инициализация He для ReLU, Xavier/Glorot для tanh/sigmoid\n",
    "            if activation_funcs[l-1] == 'relu':\n",
    "                # He initialization\n",
    "                self.parameters[f'W{l}'] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2. / layer_dims[l-1])\n",
    "            else:\n",
    "                # Xavier/Glorot initialization\n",
    "                limit = np.sqrt(6. / (layer_dims[l-1] + layer_dims[l])) \n",
    "                self.parameters[f'W{l}'] = np.random.uniform(-limit, limit, (layer_dims[l], layer_dims[l-1]))\n",
    "                \n",
    "            # Инициализация смещений нулями\n",
    "            self.parameters[f'b{l}'] = np.zeros((layer_dims[l], 1))\n",
    "            \n",
    "            # Сохранение функций активации\n",
    "            activation_name = activation_funcs[l-1]\n",
    "            if activation_name not in ACTIVATIONS:\n",
    "                raise ValueError(f\"Неизвестная функция активации: {activation_name}\")\n",
    "            self.activation_funcs[l] = ACTIVATIONS[activation_name]\n",
    "\n",
    "    def _forward(self, X):\n",
    "        \"\"\"Прямое распространение для одного мини-батча.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Входные данные мини-батча (размер: input_size x batch_size).\n",
    "\n",
    "        Returns:\n",
    "            tuple: Кортеж с выходом сети (A_last) и кешем промежуточных значений (Z, A).\n",
    "        \"\"\"\n",
    "        cache = {}\n",
    "        A = X\n",
    "        cache[\"A0\"] = A # Сохраняем входные данные как активацию 0-го слоя\n",
    "\n",
    "        # --- НАЧАЛО ВАШЕГО КОДА --- \n",
    "        # Цикл по слоям от 1 до L (num_layers - 1)\n",
    "        for l in range(1, self.num_layers):\n",
    "            # Получение параметров W и b для текущего слоя l\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            \n",
    "            # Получение функции активации для текущего слоя l\n",
    "            activation_func, _ = self.activation_funcs[l]\n",
    "            \n",
    "            # Получение активаций предыдущего слоя (A^{l-1})\n",
    "            A_prev = A # В первой итерации A = X (A0), в следующих - активации предыдущего слоя\n",
    "            \n",
    "            # Шаг 1: Вычисление линейной части (Z^l = W^l * A^{l-1} + b^l)\n",
    "            # Используйте np.dot для матричного умножения\n",
    "            Z = None # ЗАМЕНИТЬ None\n",
    "            \n",
    "            # Шаг 2: Вычисление активации (A^l = h_l(Z^l))\n",
    "            A = None # ЗАМЕНИТЬ None\n",
    "            \n",
    "            # Шаг 3: Сохранение промежуточных значений в кеш\n",
    "            # Сохраните Z, A для текущего слоя l\n",
    "            # Ключи: f'Z{l}', f'A{l}'\n",
    "            cache[f'Z{l}'] = Z\n",
    "            cache[f'A{l}'] = A\n",
    "            # A_prev уже сохранен как A{l-1} в предыдущей итерации или как A0 в начале\n",
    "            \n",
    "        # --- КОНЕЦ ВАШЕГО КОДА --- \n",
    "        \n",
    "        A_last = A # Последнее значение A - это выход сети\n",
    "        return A_last, cache\n",
    "\n",
    "    def _compute_cost(self, A_last, Y):\n",
    "        \"\"\"Вычисление функции потерь (перекрестная энтропия для бинарной классификации).\n",
    "        \n",
    "        Args:\n",
    "            A_last (np.array): Выход сети (активации последнего слоя) (размер: output_size x batch_size).\n",
    "            Y (np.array): Истинные метки (размер: output_size x batch_size).\n",
    "\n",
    "        Returns:\n",
    "            float: Значение функции потерь.\n",
    "        \"\"\"\n",
    "        m = Y.shape[1] # Размер мини-батча\n",
    "        epsilon = 1e-8 # Для численной стабильности логарифма\n",
    "        \n",
    "        # --- НАЧАЛО ВАШЕГО КОДА --- \n",
    "        # Шаг 1: Вычислите стоимость с использованием формулы перекрестной энтропии\n",
    "        # cost = -(1/m) * sum(Y * log(A_last + epsilon) + (1 - Y) * log(1 - A_last + epsilon))\n",
    "        # Используйте np.sum\n",
    "        cost = None # ЗАМЕНИТЬ None\n",
    "        # --- КОНЕЦ ВАШЕГО КОДА --- \n",
    "        \n",
    "        cost = np.squeeze(cost) # Убираем лишние размерности, если они есть\n",
    "        assert(cost.shape == ()) # Убедимся, что cost - скаляр\n",
    "        \n",
    "        return cost\n",
    "\n",
    "    def _backward(self, A_last, Y, cache):\n",
    "        \"\"\"Обратное распространение ошибки для одного мини-батча.\n",
    "\n",
    "        Args:\n",
    "            A_last (np.array): Выход сети (активации последнего слоя).\n",
    "            Y (np.array): Истинные метки.\n",
    "            cache (dict): Кеш промежуточных значений из прямого распространения (содержит Z1, A1, Z2, A2, ...).\n",
    "\n",
    "        Returns:\n",
    "            dict: Словарь с градиентами dW, db для каждого слоя (ключи: dW1, db1, dW2, db2, ...).\n",
    "        \"\"\"\n",
    "        grads = {}\n",
    "        m = Y.shape[1] # Размер мини-батча\n",
    "        L = self.num_layers - 1 # Индекс последнего слоя с параметрами (слои нумеруются с 1)\n",
    "        epsilon = 1e-8 # Для численной стабильности деления\n",
    "\n",
    "        # --- НАЧАЛО ВАШЕГО КОДА --- \n",
    "        \n",
    "        # Шаг 1: Инициализация обратного распространения (для выходного слоя L)\n",
    "        # Вычислите производную функции потерь (перекрестная энтропия) по активациям последнего слоя (dA^L)\n",
    "        # dA_last = - (Y / (A_last + epsilon) - (1 - Y) / (1 - A_last + epsilon))\n",
    "        dA_last = None # ЗАМЕНИТЬ None\n",
    "        \n",
    "        # Шаг 2: Градиенты для выходного слоя L\n",
    "        # Получите Z^L и A^{L-1} из кеша\n",
    "        Z_last = cache[f'Z{L}']\n",
    "        A_prev = cache[f'A{L-1}']\n",
    "        \n",
    "        # Получите производную функции активации для слоя L\n",
    "        _, activation_derivative = self.activation_funcs[L]\n",
    "        \n",
    "        # Вычислите dZ^L = dA^L * h_L'(Z^L)\n",
    "        dZ_last = None # ЗАМЕНИТЬ None\n",
    "        \n",
    "        # Вычислите градиенты dW^L = (1/m) * dZ^L * (A^{L-1})^T и db^L = (1/m) * sum(dZ^L, axis=1)\n",
    "        # Используйте np.dot для матричного умножения и np.sum для суммирования по оси батчей\n",
    "        grads[f'dW{L}'] = None # ЗАМЕНИТЬ None\n",
    "        grads[f'db{L}'] = None # ЗАМЕНИТЬ None (не забудьте keepdims=True для db)\n",
    "        \n",
    "        # Вычислите dA^{L-1} = (W^L)^T * dZ^L (для передачи на предыдущий слой)\n",
    "        # Получите W^L из self.parameters\n",
    "        W_last = self.parameters[f'W{L}']\n",
    "        dA_prev = None # ЗАМЕНИТЬ None\n",
    "        \n",
    "        # Шаг 3: Цикл по скрытым слоям (в обратном порядке от L-1 до 1)\n",
    "        for l in range(L-1, 0, -1):\n",
    "            # Получите Z^l и A^{l-1} из кеша\n",
    "            Z = cache[f'Z{l}']\n",
    "            A_prev_l = cache[f'A{l-1}']\n",
    "            \n",
    "            # Получите производную функции активации для слоя l\n",
    "            _, activation_derivative = self.activation_funcs[l]\n",
    "            \n",
    "            # Вычислите dZ^l = dA^{l} * h_l'(Z^l) (dA^{l} - это dA_prev из предыдущей итерации)\n",
    "            dZ = None # ЗАМЕНИТЬ None\n",
    "            \n",
    "            # Вычислите градиенты dW^l = (1/m) * dZ^l * (A^{l-1})^T и db^l = (1/m) * sum(dZ^l, axis=1)\n",
    "            grads[f'dW{l}'] = None # ЗАМЕНИТЬ None\n",
    "            grads[f'db{l}'] = None # ЗАМЕНИТЬ None (не забудьте keepdims=True для db)\n",
    "            \n",
    "            # Вычислите dA^{l-1} = (W^l)^T * dZ^l (для передачи на предыдущий слой)\n",
    "            # Получите W^l из self.parameters\n",
    "            W_current = self.parameters[f'W{l}']\n",
    "            dA_prev = None # ЗАМЕНИТЬ None\n",
    "            \n",
    "        # --- КОНЕЦ ВАШЕГО КОДА --- \n",
    "                 \n",
    "        return grads\n",
    "\n",
    "    def _update_parameters(self, grads):\n",
    "        \"\"\"Обновление весов и смещений.\n",
    "\n",
    "        Args:\n",
    "            grads (dict): Словарь с градиентами dW, db (ключи: dW1, db1, dW2, db2, ...).\n",
    "        \"\"\"\n",
    "        # --- НАЧАЛО ВАШЕГО КОДА --- \n",
    "        # Цикл по слоям от 1 до L\n",
    "        for l in range(1, self.num_layers):\n",
    "            # Обновите параметры W^l и b^l, используя градиенты из grads и скорость обучения\n",
    "            # W^l = W^l - learning_rate * dW^l\n",
    "            # b^l = b^l - learning_rate * db^l\n",
    "            self.parameters[f'W{l}'] = None # ЗАМЕНИТЬ None\n",
    "            self.parameters[f'b{l}'] = None # ЗАМЕНИТЬ None\n",
    "        # --- КОНЕЦ ВАШЕГО КОДА --- \n",
    "\n",
    "    def fit(self, X_train, Y_train, epochs, batch_size, print_cost_every=100):\n",
    "        \"\"\"Обучение нейронной сети.\n",
    "\n",
    "        Args:\n",
    "            X_train (np.array): Обучающие данные (размер: input_size x num_examples).\n",
    "            Y_train (np.array): Истинные метки для обучающих данных (размер: output_size x num_examples).\n",
    "            epochs (int): Количество эпох обучения.\n",
    "            batch_size (int): Размер мини-батча.\n",
    "            print_cost_every (int): Печатать значение функции потерь каждые N эпох (0 - не печатать).\n",
    "        \"\"\"\n",
    "        costs = [] # Для отслеживания функции потерь\n",
    "        m = X_train.shape[1] # Количество обучающих примеров\n",
    "        \n",
    "        np.random.seed(1) # Для воспроизводимости перемешивания\n",
    "        \n",
    "        # --- НАЧАЛО ВАШЕГО КОДА --- \n",
    "        # Цикл по эпохам\n",
    "        for epoch in range(epochs):\n",
    "            epoch_cost = 0.\n",
    "            \n",
    "            # Шаг 1: Перемешивание данных\n",
    "            # Создайте случайную перестановку индексов от 0 до m-1\n",
    "            permutation = None # ЗАМЕНИТЬ None\n",
    "            # Перемешайте X_train и Y_train, используя перестановку\n",
    "            shuffled_X = None # ЗАМЕНИТЬ None\n",
    "            shuffled_Y = None # ЗАМЕНИТЬ None (убедитесь, что форма Y правильная, например (1, m) для бинарной классификации)\n",
    "            \n",
    "            # Шаг 2: Определение количества мини-батчей\n",
    "            num_minibatches = m // batch_size\n",
    "            if m % batch_size != 0:\n",
    "                num_minibatches += 1\n",
    "            # Шаг 3: Цикл по мини-батчам\n",
    "            for i in range(num_minibatches):\n",
    "                # Шаг 3.1: Формирование мини-батча (mini_batch_X, mini_batch_Y)\n",
    "                # Определите начальный и конечный индексы для среза\n",
    "                start_idx = None # ЗАМЕНИТЬ None\n",
    "                end_idx = None # ЗАМЕНИТЬ None (используйте min для последнего батча)\n",
    "                # Сделайте срез из shuffled_X и shuffled_Y\n",
    "                mini_batch_X = None # ЗАМЕНИТЬ None\n",
    "                mini_batch_Y = None # ЗАМЕНИТЬ None\n",
    "                \n",
    "                # Шаг 3.2: Прямое распространение\n",
    "                # Вызовите self._forward для mini_batch_X\n",
    "                A_last, cache = None, None # ЗАМЕНИТЬ None\n",
    "                \n",
    "                # Шаг 3.3: Вычисление потерь для мини-батча\n",
    "                # Вызовите self._compute_cost\n",
    "                batch_cost = None # ЗАМЕНИТЬ None\n",
    "                # Добавьте взвешенную стоимость батча к стоимости эпохи\n",
    "                epoch_cost += batch_cost * mini_batch_X.shape[1] \n",
    "                \n",
    "                # Шаг 3.4: Обратное распространение\n",
    "                # Вызовите self._backward\n",
    "                grads = None # ЗАМЕНИТЬ None\n",
    "                \n",
    "                # Шаг 3.5: Обновление параметров\n",
    "                # Вызовите self._update_parameters\n",
    "                # ... вызов метода ...\n",
    "                \n",
    "            # Шаг 4: Вычисление средней стоимости за эпоху\n",
    "            epoch_cost /= m\n",
    "            costs.append(epoch_cost)\n",
    "            \n",
    "            # Печать стоимости (каждые print_cost_every эпох)\n",
    "            if print_cost_every > 0 and (epoch % print_cost_every == 0 or epoch == epochs - 1):\n",
    "                print(f\"Эпоха {epoch}: стоимость = {epoch_cost:.6f}\")\n",
    "        # --- КОНЕЦ ВАШЕГО КОДА --- \n",
    "                \n",
    "        return costs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Предсказание для новых данных.\n",
    "\n",
    "        Args:\n",
    "            X (np.array): Входные данные (размер: input_size x num_examples).\n",
    "\n",
    "        Returns:\n",
    "            np.array: Предсказанные метки (0 или 1 для бинарной классификации).\n",
    "        \"\"\"\n",
    "        # --- НАЧАЛО ВАШЕГО КОДА --- \n",
    "        # Шаг 1: Выполните прямое распространение, чтобы получить активации последнего слоя (A_last)\n",
    "        # Используйте self._forward\n",
    "        A_last, _ = None, None # ЗАМЕНИТЬ None\n",
    "        \n",
    "        # Шаг 2: Преобразуйте вероятности A_last в предсказания (0 или 1)\n",
    "        # Используйте порог 0.5 для бинарной классификации\n",
    "        predictions = None # ЗАМЕНИТЬ None (не забудьте .astype(int))\n",
    "        # --- КОНЕЦ ВАШЕГО КОДА --- \n",
    "        \n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Генерация и подготовка данных\n",
    "\n",
    "Этот блок остается без изменений. Он генерирует данные \"две луны\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерация данных\n",
    "X, Y = make_moons(n_samples=500, noise=0.2, random_state=42)\n",
    "\n",
    "# Визуализация данных\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, s=40, cmap=plt.cm.Spectral)\n",
    "plt.title(\"Сгенерированные данные (две луны)\")\n",
    "plt.xlabel(\"Признак 1\")\n",
    "plt.ylabel(\"Признак 2\")\n",
    "plt.show()\n",
    "\n",
    "# Преобразование данных для нашей сети\n",
    "# Транспонируем X, чтобы признаки были строками, а примеры - столбцами\n",
    "X_train = X.T\n",
    "# Преобразуем Y в вектор-строку (1 x num_examples)\n",
    "Y_train = Y.reshape(1, Y.shape[0])\n",
    "\n",
    "print(f\"Размерность X_train: {X_train.shape}\")\n",
    "print(f\"Размерность Y_train: {Y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Определение архитектуры и обучение сети\n",
    "\n",
    "Этот блок остается без изменений. Он определяет параметры сети и запускает обучение. Если ваша реализация верна, вы должны увидеть, как стоимость уменьшается со временем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры сети\n",
    "input_size = X_train.shape[0] # 2 признака\n",
    "output_size = Y_train.shape[0] # 1 выход (бинарная классификация)\n",
    "layer_dims = [input_size, 5, 3, output_size] # Архитектура: 2 -> 5 -> 3 -> 1\n",
    "activation_funcs = ['relu', 'relu', 'sigmoid'] # Функции активации для слоев 1, 2, 3\n",
    "learning_rate = 0.1\n",
    "epochs = 10000\n",
    "batch_size = 64\n",
    "\n",
    "# Создание и обучение сети\n",
    "nn = NeuralNetwork(layer_dims, activation_funcs, learning_rate)\n",
    "costs = nn.fit(X_train, Y_train, epochs, batch_size, print_cost_every=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Визуализация функции потерь\n",
    "\n",
    "Этот блок остается без изменений. Он строит график функции потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(costs)\n",
    "plt.title(\"Функция потерь во время обучения\")\n",
    "plt.xlabel(\"Эпохи\") # Обновлено\n",
    "plt.ylabel(\"Стоимость\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Визуализация границы решений\n",
    "\n",
    "Этот блок остается без изменений. Он использует вашу реализованную функцию `predict` для построения границы решений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y):\n",
    "    # Устанавливаем минимальные и максимальные значения и добавляем отступы\n",
    "    x_min, x_max = X[0, :].min() - 0.5, X[0, :].max() + 0.5\n",
    "    y_min, y_max = X[1, :].min() - 0.5, X[1, :].max() + 0.5\n",
    "    h = 0.01\n",
    "    # Генерируем сетку точек с шагом h\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    # Предсказываем значения функции для всей сетки\n",
    "    # Убедимся, что входные данные для predict имеют правильную форму\n",
    "    grid_input = np.c_[xx.ravel(), yy.ravel()].T \n",
    "    Z = model.predict(grid_input)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    # Строим контурный график и отображаем обучающие примеры\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral, alpha=0.8)\n",
    "    plt.scatter(X[0, :], X[1, :], c=y.ravel(), cmap=plt.cm.Spectral, edgecolors='k')\n",
    "    plt.title(\"Граница решений нейронной сети\")\n",
    "    plt.xlabel(\"Признак 1\")\n",
    "    plt.ylabel(\"Признак 2\")\n",
    "    plt.show()\n",
    "\n",
    "# Визуализация границы решений\n",
    "plot_decision_boundary(nn, X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Оценка точности (Accuracy)\n",
    "\n",
    "Этот блок остается без изменений. Он использует вашу реализованную функцию `predict` для вычисления точности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получаем предсказания\n",
    "predictions = nn.predict(X_train)\n",
    "\n",
    "# Сравниваем предсказания с истинными метками\n",
    "accuracy = np.mean(predictions == Y_train) * 100\n",
    "\n",
    "print(f\"Точность на обучающем наборе: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Заключение\n",
    "\n",
    "Если все блоки кода выполнились без ошибок, и вы видите график функции потерь, границу решений и значение точности, значит, вы успешно реализовали алгоритм обратного распространения ошибки!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
